{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reseted\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print('reseted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "done = False\n",
    "while not done:\n",
    "    \n",
    "    some_action = env.action_space.sample()\n",
    "    \n",
    "    observation, reward, done, info = env.step(some_action)\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    time.sleep(0.01)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space Discrete(6)\n",
      "Observation space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print('Action space {}'.format(taxi_env.action_space))\n",
    "print('Observation space {}'.format(taxi_env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state 407\n"
     ]
    }
   ],
   "source": [
    "initial_state = taxi_env.reset()\n",
    "print('Initial state {}'.format(initial_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decode state [4, 0, 1, 3]\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Decode state\", list(taxi_env.env.decode(initial_state)))\n",
    "taxi_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 407, -1, False)],\n",
       " 1: [(1.0, 307, -1, False)],\n",
       " 2: [(1.0, 407, -1, False)],\n",
       " 3: [(1.0, 407, -1, False)],\n",
       " 4: [(1.0, 407, -10, False)],\n",
       " 5: [(1.0, 407, -10, False)]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_env.env.P[initial_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('Taxi-v2')\n",
    "\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dumb agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # pick random action from a random space\n",
    "    some_action = env.action_space.sample()\n",
    "    \n",
    "    #excecute the action\n",
    "    observation, reward, done, info = env.step(some_action)\n",
    "    \n",
    "    #render the enviroment\n",
    "    env.render()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    #pause for a while\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avarage reward per move -3.941957796601674\n",
      "avarage time steps 199.51\n",
      "avarage number of penalties over episode 65.24\n"
     ]
    }
   ],
   "source": [
    "epochs, num_penalties, total_reward = 0, 0, 0\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "current_episode = 0\n",
    "\n",
    "while current_episode < num_episodes:\n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #choose random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        #make dumb move\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if reward == -10:\n",
    "            num_penalties += 1\n",
    "            \n",
    "        epochs += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "    #episode over if the time rad out or completed\n",
    "    current_episode +=1\n",
    "    env.reset()\n",
    "    \n",
    "print('avarage reward per move {}'.format(total_reward/float(epochs)))\n",
    "\n",
    "print('avarage time steps {}'.format(epochs/float(num_episodes)))\n",
    "\n",
    "print('avarage number of penalties over episode {}'.format(num_penalties/float(num_episodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading initializing and rendering the inviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make('Taxi-v2')\n",
    "\n",
    "initail_state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "# size of q table is state_space_size x action_space_size\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(q_table,env,state,epsilon):\n",
    "    \n",
    "    \"\"\"\n",
    "    Update Q-values to  the Q-learning equation.\n",
    "    \"\"\"\n",
    "    \"Creating a random number between 0, 1\"\n",
    "    if random.uniform(0,1) > epsilon:\n",
    "        action = env.action_space.sample() # select a random action\n",
    "    else:\n",
    "        action = select_optimal_action(q_table, state) # select an optimal action\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # maximum q value for the action in the next state\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    \n",
    "    next_q_value = (1- alpha) * old_q_value  + alpha * (reward + gamma * next_max)\n",
    "    \n",
    "    # updating q value\n",
    "    q[table][state] = new_q_value\n",
    "    \n",
    "    return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimal_action(q_table, state):\n",
    "    \"\"\"\n",
    "    Given a state, select the action from the action space having  the highest Q-values in the q table\n",
    "    \"\"\"\n",
    "    if np.sum(q_table[state]) == 0:\n",
    "        return random.randint(0,q_table.shape[1]-1)\n",
    "    return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(q_table, env, num_episodes, epsilon):\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            state, reward, done = q_learning_update(q_table, env, state, epsilon)\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our agent by exploition of q-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7834245759693462,\n",
       " 0.23420406595664378,\n",
       " 0.994471655741543,\n",
       " 0.12382532648560907,\n",
       " 0.4552208720881976,\n",
       " 0.6677677790611511,\n",
       " 0.2634761765657443,\n",
       " 0.7267653302820622,\n",
       " 0.263712508972648,\n",
       " 0.28042140449835695,\n",
       " 0.3670300585223062,\n",
       " 0.10441519466874938,\n",
       " 0.6833722916942587,\n",
       " 0.42284236311547807,\n",
       " 0.3289397508620874,\n",
       " 0.18197946763804596,\n",
       " 0.052667873319638736,\n",
       " 0.19331056983275496,\n",
       " 0.2942290081388437,\n",
       " 0.5864905731439731,\n",
       " 0.8667358159264932,\n",
       " 0.696802154560908,\n",
       " 0.24597809118345526,\n",
       " 0.3820463618114939,\n",
       " 0.9514875503050501,\n",
       " 0.8705055535376545,\n",
       " 0.7943870077189202,\n",
       " 0.4720550388395516,\n",
       " 0.61465292633249,\n",
       " 0.6047827232037268,\n",
       " 0.8746612102853504,\n",
       " 0.5279683393350207,\n",
       " 0.9858807751575811,\n",
       " 0.9780315622809338,\n",
       " 0.6543814799002146,\n",
       " 0.8980091717774276,\n",
       " 0.33382005764426614,\n",
       " 0.4856155048703419,\n",
       " 0.07937710793588504,\n",
       " 0.5938756362218722,\n",
       " 0.8464794631564804,\n",
       " 0.8417089017312673,\n",
       " 0.08545887604691382,\n",
       " 0.7377091819013553,\n",
       " 0.27207830498485763,\n",
       " 0.43150738800283084,\n",
       " 0.46766879470346046,\n",
       " 0.45347533328677236,\n",
       " 0.7219820148825837,\n",
       " 0.6842031783253972]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "god = []\n",
    "for i in range(50):\n",
    "    i = random.uniform(0,1)\n",
    "    god.append(i)\n",
    "god"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7651495614506941"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
